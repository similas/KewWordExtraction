{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"TextRank.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"fYWDLFFh8k_8","colab_type":"text"},"source":["# Implementation of TextRank\n","(Based on: https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)"]},{"cell_type":"markdown","metadata":{"id":"fDm73rf28k_9","colab_type":"text"},"source":["The input text is given below"]},{"cell_type":"code","metadata":{"id":"I9bfD19Q8663","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1594972475178,"user_tz":-270,"elapsed":29742,"user":{"displayName":"ali sadr","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUm6L-P05Fcco2WNrnOsvPMrEUAFE5aUg3RFLx=s64","userId":"03402713248441455199"}},"outputId":"dc44ef5e-c2b7-4fef-83a0-17fd98d4763c"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2_QEwEuU8mwP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1594972493342,"user_tz":-270,"elapsed":13150,"user":{"displayName":"ali sadr","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUm6L-P05Fcco2WNrnOsvPMrEUAFE5aUg3RFLx=s64","userId":"03402713248441455199"}},"outputId":"19a43b25-7e8b-4bb1-917f-3c1ee9b22c3b"},"source":["!pip install hazm"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting hazm\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/13/5a7074bc11d20dbbb46239349ac3f85f7edc148b4cf68e9b8c2f8263830c/hazm-0.7.0-py3-none-any.whl (316kB)\n","\u001b[K     |████████████████████████████████| 317kB 2.8MB/s \n","\u001b[?25hCollecting libwapiti>=0.2.1; platform_system != \"Windows\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/0f/1c9b49bb49821b5856a64ea6fac8d96a619b9f291d1f06999ea98a32c89c/libwapiti-0.2.1.tar.gz (233kB)\n","\u001b[K     |████████████████████████████████| 235kB 8.8MB/s \n","\u001b[?25hCollecting nltk==3.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n","\u001b[K     |████████████████████████████████| 1.4MB 13.2MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from libwapiti>=0.2.1; platform_system != \"Windows\"->hazm) (1.12.0)\n","Building wheels for collected packages: libwapiti, nltk\n","  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp36-cp36m-linux_x86_64.whl size=154544 sha256=282ba85018eadccba8294d36323cf093efe48edb06c019fa793d3af9117d65fd\n","  Stored in directory: /root/.cache/pip/wheels/66/15/54/4510dce8bb958b1cdd2c47425cbd1e1eecc0480ac9bb1fb9ab\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nltk: filename=nltk-3.3-cp36-none-any.whl size=1394473 sha256=c0a42f2b7aa42168d20256dee33ccba32f987a80ccfc1c9d397265be18bd7724\n","  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n","Successfully built libwapiti nltk\n","Installing collected packages: libwapiti, nltk, hazm\n","  Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Py6YOaOsO9TF","colab_type":"code","colab":{}},"source":["# Path\n","PosTaggerModelPath = '/content/drive/My Drive/textrankstuff/resources/POSTagger.model'\n","StopWordsPath = '/content/drive/My Drive/textrankstuff/resources/STOPWORDS.txt'\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C4urQ78cE36q","colab_type":"code","colab":{}},"source":["\n","from gensim.summarization.syntactic_unit import SyntacticUnit\n","from gensim.parsing.preprocessing import preprocess_documents\n","from gensim.utils import tokenize\n","from six.moves import xrange\n","import re\n","import logging\n","from hazm import *\n","\n","logger = logging.getLogger('summa.preprocessing.cleaner')\n","\n","try:\n","    #from pattern.en import tag\n","    from hazm import POSTagger\n","    tagger = POSTagger(model=PosTaggerModelPath)\n","    logger.info(\"'pattern' package found; tag filters are available for Persian\")\n","    HAS_PATTERN = True\n","except ImportError:\n","    #logger.info(\"'pattern' package not found; tag filters are not available for English\")\n","    logger.info(\"'pattern' package not found; tag filters are not available for Persian\")\n","    HAS_PATTERN = False\n","\n","\n","SEPARATOR = r'@'\n","RE_SENTENCE = re.compile(r'(\\S.+?[.!?])(?=\\s+|$)|(\\S.+?)(?=[\\n]|$)', re.UNICODE)  # backup (\\S.+?[.!?])(?=\\s+|$)|(\\S.+?)(?=[\\n]|$)\n","AB_SENIOR = re.compile(r'([A-Z][a-z]{1,2}\\.)\\s(\\w)', re.UNICODE)\n","AB_ACRONYM = re.compile(r'(\\.[a-zA-Z]\\.)\\s(\\w)', re.UNICODE)\n","AB_ACRONYM_LETTERS = re.compile(r'([a-zA-Z])\\.([a-zA-Z])\\.', re.UNICODE)\n","UNDO_AB_SENIOR = re.compile(r'([A-Z][a-z]{1,2}\\.)' + SEPARATOR + r'(\\w)', re.UNICODE)\n","UNDO_AB_ACRONYM = re.compile(r'(\\.[a-zA-Z]\\.)' + SEPARATOR + r'(\\w)', re.UNICODE)\n","\n","def split_sentences(text):\n","    return (sent_tokenize(text))\n","\n","def replace_abbreviations(text):\n","    return replace_with_separator(text, SEPARATOR, [AB_SENIOR, AB_ACRONYM])\n","\n","\n","def undo_replacement(sentence):\n","    return replace_with_separator(sentence, r\" \", [UNDO_AB_SENIOR, UNDO_AB_ACRONYM])\n","\n","\n","def replace_with_separator(text, separator, regexs):\n","    replacement = r\"\\1\" + separator + r\"\\2\"\n","    result = text\n","    for regex in regexs:\n","        result = regex.sub(replacement, result)\n","    return result\n","\n","\n","#def get_sentences(text):\n","#    for match in RE_SENTENCE.finditer(text):\n","#        yield match.group()\n","\n","def get_sentences(text):\n","    te = sent_tokenize(text)\n","    for each in te:\n","        yield (each)\n","\n","\n","def merge_syntactic_units(original_units, filtered_units, tags=None):\n","    units = []\n","    for i in xrange(len(original_units)):\n","        if filtered_units[i] == '':\n","            continue\n","\n","        text = original_units[i]\n","        token = filtered_units[i]\n","\n","        if tags :\n","            try:\n","                tag = tags[i][1]\n","            except:\n","                tag = None\n","        else:\n","            tag = None\n","\n","        #tag = tags[i][1] if tags else None\n","        \n","        sentence = SyntacticUnit(text, token, tag)\n","        sentence.index = i\n","\n","        units.append(sentence)\n","\n","    return units\n","\n","\n","def join_words(words, separator=\" \"):\n","    return separator.join(words)\n","\n","\n","def clean_text_by_sentences(text):\n","    \"\"\" Tokenizes a given text into sentences, applying filters and lemmatizing them.\n","    Returns a SyntacticUnit list. \"\"\"\n","    print(\"1\")\n","    original_sentences = split_sentences(text)\n","    print(\"2\")\n","    filtered_sentences = [join_words(sentence) for sentence in preprocess_documents(original_sentences)]\n","    print(\"3\")\n","    tags = clean_text_by_word(text)\n","    print(\"4\")\n","    return merge_syntactic_units(original_sentences, filtered_sentences, tags)\n","\n","\n","def clean_text_by_word(text, deacc=True):\n","    \"\"\" Tokenizes a given text into words, applying filters and lemmatizing them.\n","    Returns a dict of word -> syntacticUnit. \"\"\"\n","    text_without_acronyms = replace_with_separator(text, \"\", [AB_ACRONYM_LETTERS])\n","    original_words = list(tokenize(text_without_acronyms, to_lower=True, deacc=deacc))\n","    filtered_words = [join_words(word_list, \"\") for word_list in preprocess_documents(original_words)]\n","    if HAS_PATTERN:\n","        tags = tagger.tag(original_words) # tag needs the context of the words in the text\n","    else:\n","        tags = None\n","    units = merge_syntactic_units(original_words, filtered_words, tags)\n","    return dict((unit.text,unit) for unit in units)\n","\n","\n","def tokenize_by_word(text):\n","    text_without_acronyms = replace_with_separator(text, \"\", [AB_ACRONYM_LETTERS])\n","    return tokenize(text_without_acronyms, to_lower=True, deacc=True)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"62BVvs7NCqFJ","colab_type":"code","colab":{}},"source":["import sys\n","sys.path.append('/content/drive/My Drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n2dKHl3Q8k_-","colab_type":"code","colab":{}},"source":["b = '''\n","داوری فرایندی است که به موجب آن اختلاف طرفین نسبت به حقوق و تکالیف قانونی خود از طریق انتخاب یک یا چند نفر داور به جای دادگاه حل وفصل گردیده و رای لازم الاجرا صادر می گردد. در فصل اول کلیاتی راجع به مفهوم و مقایسه‌ی داوری با مفاهیم مشابه و اقسام داوری،داوری در مسائل خاص، مبانی حقوقی داوری در حقوق ایران بیان شده است و در فصل دوم  به بررسی ماهیت و شرایط نصب داور و ممنوعیت‌های مرتبط با نصب داور پرداخته‌‌ایم و درفصل سوم به  بررسی آثار نصب داور پرداخته شده است.النهایه به این نتیجه رسیده‌ایم که ماهیت نصب داور بانظریه مختلط تطبیق بیشتری دارد  و در خصوص شرایط نصب داور می‌توان به موارد زیر اشاره کرد: داور‌پذیری، استقلال و بی‌طرفی داور اشاره کرد و در خصوص ممنوعیت نصب داور می‌توان به ممنوعیت‌‌های مرتبط با داور و ممنوعیت‌‌های مرتبط با موضوع اختلاف و همچنین ممنوعیت‌های مرتبط با طرفین اختلاف اشاره کرد و در خصوص آثار نصب داور می‌توان گفت که هم برای طرفین اختلاف و هم برای دادگاه‌ها و هم برای داور تعهداتی را ایجاد می‌کند و از جمله‌ی آن‌ها می‌توان به لازم التباع بودن رأی داور توسط طرفین و اعمال قاعده‌ای امر قضاوت شده اشاره کرد و برای دادگاه‌ها مهم‌ترین اثرآن این است از دادگاه‌ها سلب صلاحیت می‌کند و برای داور الزام به رسیدگی و صدور رأی را ایجاد می‌کند\n","\n"," '''\n","\n","import math\n","import numpy as np\n","from hazm import *\n","# from textcleaner import clean_text_by_sentences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Zh7lzmmB8lAC","colab_type":"code","colab":{}},"source":["\n","def kwextractor(text):\n","    \n","\ttagger = POSTagger(model=PosTaggerModelPath)\n","\ttagged = tagger.tag(word_tokenize(text))\n","\tlemmatizer = Lemmatizer()\n","\tadjective_tags = ['JJ','JJR','JJS']\n","\tlemmatized_text_0 = []\n","\tfor word in tagged:\n","\t    if word[1] in adjective_tags:\n","\t        lemmatized_text_0.append(str(lemmatizer.lemmatize(word[0],pos=\"a\")))\n","\t    else:\n","\t        lemmatized_text_0.append(str(lemmatizer.lemmatize(word[0])))\n","            \n","# \tprint(lemmatized_text_0)\n","    \n","\tlemmatized_text = []\n","\tfor word in lemmatized_text_0:\n","\t\tif \"#\" in word:\n","\t\t\tsharpIdx = word.find(\"#\")\n","\t\t\tif sharpIdx != 0:\n","\t\t\t\tnew = word[:sharpIdx]\n","\t\t\telse:\n","\t\t\t\tnew = word[sharpIdx+1:]\n","\t\telse:\n","\t\t\tlemmatized_text.append(word)\n","        \n","# \tprint()\n","# \tprint(\"-----------------------------------------\")\n","# \tprint()\n","# \tprint(lemmatized_text)\n","    \n","\tPOS_tag = tagger.tag(lemmatized_text)\n","\tstopwords = []\n","\tstopword_file = open(StopWordsPath , \"r\")\n","\tlots_of_stopwords = []\n","\tfor line in stopword_file.readlines():\n","\t    lots_of_stopwords.append(str(line.strip()))\n","\n","\tstopwords_plus = []\n","\tstopwords_plus = stopwords + lots_of_stopwords\n","\tstopwords_plus = set(stopwords_plus)\n","# \tprint(stopwords_plus)\n","    \n","\tprocessed_text = []\n","\tfor word in lemmatized_text:\n","\t    if word not in stopwords_plus:\n","# \t        print(word)\n","\t        processed_text.append(word)\n","\tvocabulary = list(set(processed_text))\n","\tvocab_len = len(vocabulary)\n","\tweighted_edge = np.zeros((vocab_len,vocab_len),dtype=np.float32)\n","\tscore = np.zeros((vocab_len),dtype=np.float32)\n","\twindow_size = 3\n","\tcovered_coocurrences = []\n","\tfor i in range(0,vocab_len):\n","\t    score[i]=1\n","\t    for j in range(0,vocab_len):\n","\t        if j==i:\n","\t            weighted_edge[i][j]=0\n","\t        else:\n","\t            for window_start in range(0,(len(processed_text)-window_size)):\n","\t                \n","\t                window_end = window_start+window_size\n","\t                \n","\t                window = processed_text[window_start:window_end]\n","\t                \n","\t                if (vocabulary[i] in window) and (vocabulary[j] in window):\n","\t                    \n","\t                    index_of_i = window_start + window.index(vocabulary[i])\n","\t                    index_of_j = window_start + window.index(vocabulary[j])\n","\t                    if [index_of_i,index_of_j] not in covered_coocurrences:\n","\t                        weighted_edge[i][j]+=1/math.fabs(index_of_i-index_of_j)\n","\t                        covered_coocurrences.append([index_of_i,index_of_j])\n","\n","\tinout = np.zeros((vocab_len),dtype=np.float32)\n","\n","\tfor i in range(0,vocab_len):\n","\t    for j in range(0,vocab_len):\n","\t        inout[i]+=weighted_edge[i][j]\n","\tMAX_ITERATIONS = 50\n","\td=0.85\n","\tthreshold = 0.0001 #convergence threshold\n","\n","\tfor iter in range(0,MAX_ITERATIONS):\n","\t    prev_score = np.copy(score)\n","\t    \n","\t    for i in range(0,vocab_len):\n","\t        \n","\t        summation = 0\n","\t        for j in range(0,vocab_len):\n","\t            if weighted_edge[i][j] != 0:\n","\t                summation += (weighted_edge[i][j]/inout[j])*score[j]\n","\t                \n","\t        score[i] = (1-d) + d*(summation)\n","\t    \n","\t    if np.sum(np.fabs(prev_score-score)) <= threshold: #convergence condition\n","\t        break\n","\n","\tphrases = []\n","\n","\tphrase = \" \"\n","\tfor word in lemmatized_text:\n","\t    \n","\t    if word in stopwords_plus:\n","\t        if phrase!= \" \":\n","\t            phrases.append(str(phrase).strip().split())\n","\t        phrase = \" \"\n","\t    elif word not in stopwords_plus:\n","\t        phrase+=str(word)\n","\t        phrase+=\" \"\n","\n","\tunique_phrases = []\n","\n","\tfor phrase in phrases:\n","\t    if phrase not in unique_phrases:\n","\t        unique_phrases.append(phrase)\n","\n","\tfor word in vocabulary:\n","\t    for phrase in unique_phrases:\n","\t        if (word in phrase) and ([word] in unique_phrases) and (len(phrase)>1):\n","\t            unique_phrases.remove([word])     \n","  \n","\tphrase_scores = []\n","\tkeywords = []\n","\tfor phrase in unique_phrases:\n","\t    phrase_score=0\n","\t    keyword = ''\n","\t    for word in phrase:\n","\t        keyword += str(word)\n","\t        keyword += \" \"\n","\t        phrase_score+=score[vocabulary.index(word)]\n","\t    phrase_scores.append(phrase_score)\n","\t    keywords.append(keyword.strip())\n","\n","\ti=0\n","\twords_with_scores = []\n","\tfor keyword in keywords:\n","\t    tup = (keyword,phrase_scores[i])\n","\t    words_with_scores.append(tup)\n","\t    i+=1\n","\n","\n","\tsortedKeywords = sorted(words_with_scores, key=lambda x: x[1], reverse=True)\n","\tsortedKeywords = [x[0] for x in sortedKeywords]\n","  # print(\"*\" * 100)\n","\tprint(sortedKeywords)\n","\t# sorted_index = np.argsort(phrase_scores)\n","\t# sorted_keywords = []\n","\t# for idx in range(len())\n","\t# print(len(sorted_index))\n","\tkeywords_num = 10\n","\tfinal_keywords = []\n","\tfor word in sortedKeywords:\n","\t\tif \"#\" in word:\n","\t\t\tsharpIdx = word.find(\"#\")\n","\t\t\tif sharpIdx != 0:\n","\t\t\t\tnew = word[:sharpIdx]\n","\t\t\telse:\n","\t\t\t\tnew = word[sharpIdx+1:]\n","\t\t\tif new not in stopwords_plus: \n","\t\t\t\tfinal_keywords.append(new)\n","\t\telse:\n","\t\t\tfinal_keywords.append(word)\n","\t# print(len(final_keywords))\n","\treturn final_keywords  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bRvG4cdQ8lAE","colab_type":"text"},"source":["# ALL TOGETHER"]},{"cell_type":"code","metadata":{"id":"2favp_We8lAF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":877},"executionInfo":{"status":"ok","timestamp":1592991810899,"user_tz":-270,"elapsed":3986,"user":{"displayName":"ali sadr","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUm6L-P05Fcco2WNrnOsvPMrEUAFE5aUg3RFLx=s64","userId":"03402713248441455199"}},"outputId":"e9e8e6e9-5bde-4673-d2e4-8bd9b62592b2"},"source":["extracted_keywordss = kwextractor(b)\n","print(\"==================PREDICTED=================\")\n","for wo in extracted_keywordss:\n","    print(wo)\n","\n","real = \"\"\"\n","حل اختلاف,داور,ان‍ت‍ص‍اب‌,صلاحیت حرفه‌ای,بی‌طرفی,استقلال قضایی,اصول حقوقی\n","\"\"\"\n","\n","print(\"==================REAL====================\")\n","# real = real.split(\",\")\n","# for w in real:\n","#     print(w)\n","# print(real)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['خصوص ممنوعیت نصب داور می\\u200cتوان', 'خصوص شرایط نصب داور می\\u200cتوان', 'خصوص آثار نصب داور می\\u200cتوان', 'ماهیت نصب داور بانظریه مختلط تطبیق', 'بررسی آثار نصب داور', 'شرایط نصب داور', 'نصب داور پرداخته', 'بی\\u200cطرف داور اشاره', 'مبانی حقوق داور', 'رأی داور', 'مقایسه داور', 'داور الزام', 'نفر داور', 'داور تعهدات', 'اقسام داور', 'داور فرایند', 'طرفین اختلاف اشاره', 'اختلاف طرفین نسبت', 'دادگاه حل وفصل گردیده', 'طرفین اختلاف', 'دادگاه سلب صلاحیت', 'دادگاه مهم اثرآن', 'ممنوعیت مرتبط', 'حقوق ایران بیان', 'لازم الاجرا صادر', 'ممنوعیت\\u200c\\u200cهای مرتبط', 'اعمال قاعده امر قضاوت', 'موضوع اختلاف', 'بررسی ماهیت', 'لازم التباع', 'صدور رأی', 'کلیات راجع', 'تکالیف قانون', 'مفاهیم مشابه', 'فصل', 'مسائل خاص', 'مفهوم', 'رسیدگی', 'انتخاب', 'ایجاد', 'جمله', 'استقلال', 'داور\\u200cپذیری', 'درفصل', 'نتیجه', 'موجب', 'النهایه']\n","==================PREDICTED=================\n","خصوص ممنوعیت نصب داور می‌توان\n","خصوص شرایط نصب داور می‌توان\n","خصوص آثار نصب داور می‌توان\n","ماهیت نصب داور بانظریه مختلط تطبیق\n","بررسی آثار نصب داور\n","شرایط نصب داور\n","نصب داور پرداخته\n","بی‌طرف داور اشاره\n","مبانی حقوق داور\n","رأی داور\n","مقایسه داور\n","داور الزام\n","نفر داور\n","داور تعهدات\n","اقسام داور\n","داور فرایند\n","طرفین اختلاف اشاره\n","اختلاف طرفین نسبت\n","دادگاه حل وفصل گردیده\n","طرفین اختلاف\n","دادگاه سلب صلاحیت\n","دادگاه مهم اثرآن\n","ممنوعیت مرتبط\n","حقوق ایران بیان\n","لازم الاجرا صادر\n","ممنوعیت‌‌های مرتبط\n","اعمال قاعده امر قضاوت\n","موضوع اختلاف\n","بررسی ماهیت\n","لازم التباع\n","صدور رأی\n","کلیات راجع\n","تکالیف قانون\n","مفاهیم مشابه\n","فصل\n","مسائل خاص\n","مفهوم\n","رسیدگی\n","انتخاب\n","ایجاد\n","جمله\n","استقلال\n","داور‌پذیری\n","درفصل\n","نتیجه\n","موجب\n","النهایه\n","==================REAL====================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TfYrUMtE8lAJ","colab_type":"code","colab":{}},"source":["import json\n","json_file_name = \"data1.json\"\n","result_file = \"text_rank.json\"\n","result_list = []\n","result_dict = {}\n","# print(\"111\")\n","\n","# with open(json_file_name) as json_file, open(result_file, 'w') as resfile:\n","#     data = json.load(json_file)\n","#     x = data['articles'][16]['extract']\n","#     kwextractor(x)\n","\n","\n","\n","\n","\n","\n","with open(json_file_name) as json_file, open(result_file, 'w') as resfile:\n","    data = json.load(json_file)\n","    for ii,article in enumerate(data['articles'][:50]):\n","        result_dict['#'] = article['#']\n","        result_dict['extract'] = article['extract']\n","        result_dict['FirstLevelSubject'] = article['firstlevelsubject']\n","        result_dict['ID'] = article['id']\n","        print(f\"{ii} : {result_dict['ID']}\")\n","        result_dict['Keywords'] = article['keywords']\n","        result_dict['TextRank-Keywords'] = \" , \".join(kwextractor(article['extract']))\n","        result_dict['MAJOR'] = article['major']\n","        result_dict['SUBJECT'] = article['subject']\n","        result_dict['TITLE'] = article['title']\n","        result_list.append(result_dict)\n","    result_list = {'articles': result_list}\n","    j = json.dumps(result_list)\n","    resfile.write(json.dumps(result_list, indent=4, sort_keys=True, ensure_ascii=False))\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2pY5RTfr8lAN","colab_type":"text"},"source":["# ---------------------\n","# STEP BY STEP\n","# ---------------------"]},{"cell_type":"markdown","metadata":{"id":"sdYzWPfe8lAN","colab_type":"text"},"source":["### POS Tagging For Lemmatization\n","\n","NLTK is again used for <b>POS tagging</b> the input text so that the words can be lemmatized based on their POS tags.\n","\n","Description of POS tags: \n","\n","\n","http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"]},{"cell_type":"code","metadata":{"id":"4Cl457yF8lAO","colab_type":"code","colab":{}},"source":["#nltk.download('averaged_perceptron_tagger')\n","  \n","\n","tagger = POSTagger(model='resources/POSTagger.model')\n","tagged = tagger.tag(word_tokenize(text))\n","\n","print (\"Tokenized Text with POS tags: \\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7QMwXKZE8lAR","colab_type":"text"},"source":["### Lemmatization\n","\n","The tokenized text (mainly the nouns and adjectives) is normalized by <b>lemmatization</b>.\n","In lemmatization different grammatical counterparts of a word will be replaced by single\n","basic lemma. For example, 'glasses' may be replaced by 'glass'. \n","\n","Details about lemmatization: \n","    \n","https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"]},{"cell_type":"code","metadata":{"id":"oO5sKfOc8lAS","colab_type":"code","colab":{}},"source":["#nltk.download('wordnet')\n","\n","# wordnet_lemmatizer = WordNetLemmatizer()\n","lemmatizer = Lemmatizer()\n","\n","adjective_tags = ['JJ','JJR','JJS']\n","\n","lemmatized_text = []\n","\n","for word in tagged:\n","    if word[1] in adjective_tags:\n","        lemmatized_text.append(str(lemmatizer.lemmatize(word[0],pos=\"a\")))\n","    else:\n","        lemmatized_text.append(str(lemmatizer.lemmatize(word[0]))) #default POS = noun\n","        \n","print (\"Text tokens after lemmatization of adjectives and nouns: \\n\")\n","print (lemmatized_text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VA7yMwwA8lAV","colab_type":"text"},"source":["### POS tagging for Filtering\n","\n","The <b>lemmatized text</b> is <b>POS tagged</b> here. The tags will be used for filtering later on."]},{"cell_type":"code","metadata":{"id":"LqHtdylI8lAV","colab_type":"code","colab":{}},"source":["POS_tag = tagger.tag(lemmatized_text)\n","\n","print (\"Lemmatized text with POS tags: \\n\")\n","print (POS_tag)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ke3lm_cW8lAY","colab_type":"text"},"source":["## POS Based Filtering\n","\n","Any word from the lemmatized text, which isn't a noun, adjective, or gerund (or a 'foreign word'), is here\n","considered as a <b>stopword</b> (non-content). This is based on the assumption that usually keywords are noun,\n","adjectives or gerunds. \n","\n","Punctuations are added to the stopword list too."]},{"cell_type":"code","metadata":{"id":"UO5ShXTq8lAY","colab_type":"code","colab":{}},"source":["# stopwords = []\n","\n","# wanted_POS = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','VBG','FW'] \n","\n","# for word in POS_tag:\n","#     if word[1] not in wanted_POS:\n","#         stopwords.append(word[0])\n","\n","# punctuations = list(str(string.punctuation))\n","\n","# stopwords = stopwords + punctuations"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mOkt7QMT8lAb","colab_type":"text"},"source":["### Complete stopword generation\n","\n","Even if we remove the aforementioned stopwords, still some extremely common nouns, adjectives or gerunds may\n","remain which are very bad candidates for being keywords (or part of it). \n","\n","An external file constituting a long list of stopwords is loaded and all the words are added with the previous\n","stopwords to create the final list 'stopwords-plus' which is then converted into a set. \n","\n","(Source of stopwords data: https://www.ranks.nl/stopwords)\n","\n","Stopwords-plus constitute the sum total of all stopwords and potential phrase-delimiters. \n","\n","(The contents of this set will be later used to partition the lemmatized text into n-gram phrases. But, for now, I will simply remove the stopwords, and work with a 'bag-of-words' approach. I will be developing the graph using unigram texts as vertices)"]},{"cell_type":"code","metadata":{"id":"g9DKicyb8lAb","colab_type":"code","colab":{}},"source":["stopwords = []\n","stopword_file = open(\"resources/STOPWORDS.txt\", \"r\")\n","#Source = https://www.ranks.nl/stopwords\n","\n","lots_of_stopwords = []\n","\n","for line in stopword_file.readlines():\n","    lots_of_stopwords.append(str(line.strip()))\n","\n","stopwords_plus = []\n","stopwords_plus = stopwords + lots_of_stopwords\n","stopwords_plus = set(stopwords_plus)\n","stopwords_plus\n","#Stopwords_plus contain total set of all stopwords"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TavihVci8lAe","colab_type":"text"},"source":["### Removing Stopwords \n","\n","Removing stopwords from lemmatized_text. \n","Processeced_text condtains the result."]},{"cell_type":"code","metadata":{"id":"8PA5BqA48lAe","colab_type":"code","colab":{}},"source":["processed_text = []\n","for word in lemmatized_text:\n","#     print(word)\n","    if word not in stopwords_plus:\n","        processed_text.append(word)\n","\n","for w in processed_text:\n","    print(w)\n","# print (processed_text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1FZZI2y18lAh","colab_type":"text"},"source":["## Vocabulary Creation\n","\n","Vocabulary will only contain unique words from processed_text."]},{"cell_type":"code","metadata":{"id":"24Fb7swn8lAj","colab_type":"code","colab":{}},"source":["vocabulary = list(set(processed_text))\n","for w in vocabulary:\n","    print(w)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HcoCmTN-8lAn","colab_type":"text"},"source":["### Building Graph\n","\n","TextRank is a graph based model, and thus it requires us to build a graph. Each words in the vocabulary will serve as a vertex for graph. The words will be represented in the vertices by their index in vocabulary list.  \n","\n","The weighted_edge matrix contains the information of edge connections among all vertices.\n","I am building wieghted undirected edges.\n","\n","weighted_edge[i][j] contains the weight of the connecting edge between the word vertex represented by vocabulary index i and the word vertex represented by vocabulary j.\n","\n","If weighted_edge[i][j] is zero, it means no edge connection is present between the words represented by index i and j.\n","\n","There is a connection between the words (and thus between i and j which represents them) if the words co-occur within a window of a specified 'window_size' in the processed_text.\n","\n","The value of the weighted_edge[i][j] is increased by (1/(distance between positions of words currently represented by i and j)) for every connection discovered between the same words in different locations of the text. \n","\n","The covered_coocurrences list (which is contain the list of pairs of absolute positions in processed_text of the words whose coocurrence at that location is already checked) is managed so that the same two words located in the same positions in processed_text are not repetitively counted while sliding the window one text unit at a time.\n","\n","The score of all vertices are intialized to one. \n","\n","Self-connections are not considered, so weighted_edge[i][i] will be zero."]},{"cell_type":"code","metadata":{"id":"dNDSvemk8lAn","colab_type":"code","colab":{}},"source":["import numpy as np\n","import math\n","vocab_len = len(vocabulary)\n","\n","weighted_edge = np.zeros((vocab_len,vocab_len),dtype=np.float32)\n","\n","score = np.zeros((vocab_len),dtype=np.float32)\n","window_size = 3\n","covered_coocurrences = []\n","\n","for i in range(0,vocab_len):\n","    score[i]=1\n","    for j in range(0,vocab_len):\n","        if j==i:\n","            weighted_edge[i][j]=0\n","        else:\n","            for window_start in range(0,(len(processed_text)-window_size)):\n","                \n","                window_end = window_start+window_size\n","                \n","                window = processed_text[window_start:window_end]\n","                \n","                if (vocabulary[i] in window) and (vocabulary[j] in window):\n","                    \n","                    index_of_i = window_start + window.index(vocabulary[i])\n","                    index_of_j = window_start + window.index(vocabulary[j])\n","                    \n","                    # index_of_x is the absolute position of the xth term in the window \n","                    # (counting from 0) \n","                    # in the processed_text\n","                      \n","                    if [index_of_i,index_of_j] not in covered_coocurrences:\n","                        weighted_edge[i][j]+=1/math.fabs(index_of_i-index_of_j)\n","                        covered_coocurrences.append([index_of_i,index_of_j])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nQ6abfc48lAq","colab_type":"text"},"source":["### Calculating weighted summation of connections of a vertex\n","\n","inout[i] will contain the sum of all the undirected connections\\edges associated withe the vertex represented by i."]},{"cell_type":"code","metadata":{"id":"U4lFT3Kr8lAq","colab_type":"code","colab":{}},"source":["inout = np.zeros((vocab_len),dtype=np.float32)\n","\n","for i in range(0,vocab_len):\n","    for j in range(0,vocab_len):\n","        inout[i]+=weighted_edge[i][j]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R78Gqlux8lAu","colab_type":"text"},"source":["### Scoring Vertices\n","\n","The formula used for scoring a vertex represented by i is:\n","\n","score[i] = (1-d) + d x [ Summation(j) ( (weighted_edge[i][j]/inout[j]) x score[j] ) ] where j belongs to the list of vertieces that has a connection with i. \n","\n","d is the damping factor.\n","\n","The score is iteratively updated until convergence. "]},{"cell_type":"code","metadata":{"id":"q_zCpQN58lAu","colab_type":"code","colab":{}},"source":["MAX_ITERATIONS = 50\n","d=0.85\n","threshold = 0.0001 #convergence threshold\n","\n","for iter in range(0,MAX_ITERATIONS):\n","    prev_score = np.copy(score)\n","    \n","    for i in range(0,vocab_len):\n","        \n","        summation = 0\n","        for j in range(0,vocab_len):\n","            if weighted_edge[i][j] != 0:\n","                summation += (weighted_edge[i][j]/inout[j])*score[j]\n","                \n","        score[i] = (1-d) + d*(summation)\n","    \n","    if np.sum(np.fabs(prev_score-score)) <= threshold: #convergence condition\n","        print(\"Converging at iteration \"+str(iter)+\"....\")\n","        break\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tEmqnbuJ8lAx","colab_type":"code","colab":{}},"source":["for i in range(0,vocab_len):\n","    print(\"Score of \"+vocabulary[i]+\": \"+str(score[i]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GnOnjDVi8lA0","colab_type":"text"},"source":["### Phrase Partiotioning\n","\n","Paritioning lemmatized_text into phrases using the stopwords in it as delimeters.\n","The phrases are also candidates for keyphrases to be extracted. "]},{"cell_type":"code","metadata":{"id":"tl8Y5idT8lA0","colab_type":"code","colab":{}},"source":["phrases = []\n","\n","phrase = \" \"\n","for word in lemmatized_text:\n","    \n","    if word in stopwords_plus:\n","        if phrase!= \" \":\n","            phrases.append(str(phrase).strip().split())\n","        phrase = \" \"\n","    elif word not in stopwords_plus:\n","        phrase+=str(word)\n","        phrase+=\" \"\n","\n","print(\"Partitioned Phrases (Candidate Keyphrases): \\n\")\n","print(phrases)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DaFqu8Br8lA3","colab_type":"text"},"source":["### Create a list of unique phrases.\n","\n","Repeating phrases\\keyphrase candidates has no purpose here, anymore. "]},{"cell_type":"code","metadata":{"id":"JKyRkb068lA3","colab_type":"code","colab":{}},"source":["unique_phrases = []\n","\n","for phrase in phrases:\n","    if phrase not in unique_phrases:\n","        unique_phrases.append(phrase)\n","\n","print(\"Unique Phrases (Candidate Keyphrases): \\n\")\n","print(unique_phrases)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HSGaT-RS8lA5","colab_type":"text"},"source":["### Thinning the list of candidate-keyphrases.\n","\n","Removing single word keyphrases-candidates that are present multi-word alternatives. "]},{"cell_type":"code","metadata":{"id":"HU7IVS698lA6","colab_type":"code","colab":{}},"source":["for word in vocabulary:\n","    #print word\n","    for phrase in unique_phrases:\n","        if (word in phrase) and ([word] in unique_phrases) and (len(phrase)>1):\n","            #if len(phrase)>1 then the current phrase is multi-worded.\n","            #if the word in vocabulary is present in unique_phrases as a single-word-phrase\n","            # and at the same time present as a word within a multi-worded phrase,\n","            # then I will remove the single-word-phrase from the list.\n","            unique_phrases.remove([word])\n","            \n","print(\"Thinned Unique Phrases (Candidate Keyphrases): \\n\")\n","print(unique_phrases)    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M-Huz-Mn8lA9","colab_type":"text"},"source":["### Scoring Keyphrases\n","\n","Scoring the phrases (candidate keyphrases) and building up a list of keyphrases\\keywords\n","by listing untokenized versions of tokenized phrases\\candidate-keyphrases.\n","Phrases are scored by adding the score of their members (words\\text-units that were ranked by the graph algorithm)\n"]},{"cell_type":"code","metadata":{"id":"Av4nbPQl8lA-","colab_type":"code","colab":{}},"source":["phrase_scores = []\n","keywords = []\n","for phrase in unique_phrases:\n","    phrase_score=0\n","    keyword = ''\n","    for word in phrase:\n","        keyword += str(word)\n","        keyword += \" \"\n","        phrase_score+=score[vocabulary.index(word)]\n","    phrase_scores.append(phrase_score)\n","    keywords.append(keyword.strip())\n","\n","i=0\n","words_with_scores = []\n","for keyword in keywords:\n","#     print(words_with_scores)\n","    tup = (keyword,phrase_scores[i])\n","    words_with_scores.append(tup)\n","#     print (\"Keyword: '\"+str(keyword)+\"', Score: \"+str(phrase_scores[i]))\n","    i+=1\n"," \n","# print(len(words_with_scores))\n","# print(word_with_scores)\n","for i,ws in enumerate(words_with_scores):\n","    print(f\"{i}\",ws)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6V4b6W0i8lBA","colab_type":"text"},"source":["### Ranking Keyphrases\n","\n","Ranking keyphrases based on their calculated scores. Displaying top keywords_num no. of keyphrases."]},{"cell_type":"code","metadata":{"id":"5SZjE2Vp8lBA","colab_type":"code","colab":{}},"source":["sorted_index = np.flip(np.argsort(phrase_scores),0)\n","\n","print(phrase_scores)\n","\n","keywords_num = 10\n","\n","print(\"Keywords:\\n\")\n","\n","for w in keywords:\n","    print(w)\n","# for i in range(0,keywords_num):\n","#     try:\n","#         print(str(i),str(keywords[sorted_index[i]])+\", \", end=' ')\n","#     except:\n","#         continue"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J2meryCM8lBD","colab_type":"text"},"source":["# Input:\n","\n","Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types of systems and systems of mixed types.\n","\n","# Extracted Keywords:\n","\n","* minimal supporting set,  \n","* minimal generating set,  \n","* minimal set,  \n","* linear diophantine equation,  \n","* nonstrict inequations,  \n","* strict inequations,  \n","* system,  \n","* linear constraint,  \n","* solution,  \n","* upper bound, \n"]},{"cell_type":"code","metadata":{"id":"_UE9o_Hg8lBD","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6DibfEem8lBG","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}