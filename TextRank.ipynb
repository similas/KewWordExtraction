{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of TextRank\n",
    "(Based on: https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input text is given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source of text:\n",
    "#https://www.researchgate.net/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents\n",
    "\n",
    "# text = \"سلام.حالت چطوره ..آنها سلام وووو..\"\n",
    "\n",
    "text = '''\n",
    "در نمایشگاه کتاب بندرعباس آثار دونالد ترامپ در حوزه کتاب‌های موفقیت به وفور عرضه می‌شود. این در حالی است که رئیس‌جمهور آمریکا مراکز فرهنگی ما را تهدید می‌کند اما ما کتاب‌های او را تبلیغ می‌کنیم!]\n",
    "\n",
    " '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Text Data\n",
    "\n",
    "The raw input text is cleaned off non-printable characters (if any) and turned into lower case.\n",
    "The processed input text is then tokenized using NLTK library functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "[Original unit: 'در نمایشگاه کتاب بندرعباس آثار دونالد ترامپ در حوزه کتاب‌های موفقیت به وفور عرضه می‌شود.' *-*-*-* Processed unit: 'نمایشگاه کتاب بندرعباس آثار دونالد ترامپ حوزه کتاب‌های موفقیت وفور عرضه می‌شود', Original unit: 'این در حالی است که رئیس‌جمهور آمریکا مراکز فرهنگی ما را تهدید می‌کند اما ما کتاب‌های او را تبلیغ می‌کنیم!]' *-*-*-* Processed unit: 'این حالی است رئیس‌جمهور آمریکا مراکز فرهنگی تهدید می‌کند اما کتاب‌های تبلیغ می‌کنیم']\n"
     ]
    }
   ],
   "source": [
    "from hazm import *\n",
    "from textcleaner import clean_text_by_sentences\n",
    "from textcleaner import clean_text_by_sentences\n",
    "\n",
    "print(clean_text_by_sentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging For Lemmatization\n",
    "\n",
    "NLTK is again used for <b>POS tagging</b> the input text so that the words can be lemmatized based on their POS tags.\n",
    "\n",
    "Description of POS tags: \n",
    "\n",
    "\n",
    "http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text with POS tags: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')\n",
    "  \n",
    "\n",
    "tagger = POSTagger(model='resources/POSTagger.model')\n",
    "tagged = tagger.tag(word_tokenize(text))\n",
    "\n",
    "print (\"Tokenized Text with POS tags: \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "The tokenized text (mainly the nouns and adjectives) is normalized by <b>lemmatization</b>.\n",
    "In lemmatization different grammatical counterparts of a word will be replaced by single\n",
    "basic lemma. For example, 'glasses' may be replaced by 'glass'. \n",
    "\n",
    "Details about lemmatization: \n",
    "    \n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text tokens after lemmatization of adjectives and nouns: \n",
      "\n",
      "['در', 'نمایشگاه', 'کتاب', 'بندرعباس', 'آثار', 'دونالد', 'ترامپ', 'در', 'حوزه', 'کتاب', 'موفقیت', 'به', 'وفور', 'عرضه', 'شد#شو', '.', 'این', 'در', 'حال', '#است', 'که', 'رئیس\\u200cجمهور', 'آمریکا', 'مراکز', 'فرهنگ', 'ما', 'را', 'تهدید', 'کرد#کن', 'اما', 'ما', 'کتاب', 'او', 'را', 'تبلیغ', 'کرد#کن', '!', ']']\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('wordnet')\n",
    "\n",
    "# wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "adjective_tags = ['JJ','JJR','JJS']\n",
    "\n",
    "lemmatized_text = []\n",
    "\n",
    "for word in tagged:\n",
    "    if word[1] in adjective_tags:\n",
    "        lemmatized_text.append(str(lemmatizer.lemmatize(word[0],pos=\"a\")))\n",
    "    else:\n",
    "        lemmatized_text.append(str(lemmatizer.lemmatize(word[0]))) #default POS = noun\n",
    "        \n",
    "print (\"Text tokens after lemmatization of adjectives and nouns: \\n\")\n",
    "print (lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging for Filtering\n",
    "\n",
    "The <b>lemmatized text</b> is <b>POS tagged</b> here. The tags will be used for filtering later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized text with POS tags: \n",
      "\n",
      "[('در', 'P'), ('نمایشگاه', 'Ne'), ('کتاب', 'Ne'), ('بندرعباس', 'N'), ('آثار', 'Ne'), ('دونالد', 'N'), ('ترامپ', 'N'), ('در', 'P'), ('حوزه', 'Ne'), ('کتاب', 'Ne'), ('موفقیت', 'N'), ('به', 'P'), ('وفور', 'Ne'), ('عرضه', 'N'), ('شد#شو', 'V'), ('.', 'PUNC'), ('این', 'DET'), ('در', 'P'), ('حال', 'N'), ('#است', 'V'), ('که', 'CONJ'), ('رئیس\\u200cجمهور', 'Ne'), ('آمریکا', 'N'), ('مراکز', 'Ne'), ('فرهنگ', 'Ne'), ('ما', 'PRO'), ('را', 'POSTP'), ('تهدید', 'N'), ('کرد#کن', 'V'), ('اما', 'CONJ'), ('ما', 'PRO'), ('کتاب', 'Ne'), ('او', 'PRO'), ('را', 'POSTP'), ('تبلیغ', 'N'), ('کرد#کن', 'V'), ('!', 'PUNC'), (']', 'PUNC')]\n"
     ]
    }
   ],
   "source": [
    "POS_tag = tagger.tag(lemmatized_text)\n",
    "\n",
    "print (\"Lemmatized text with POS tags: \\n\")\n",
    "print (POS_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Based Filtering\n",
    "\n",
    "Any word from the lemmatized text, which isn't a noun, adjective, or gerund (or a 'foreign word'), is here\n",
    "considered as a <b>stopword</b> (non-content). This is based on the assumption that usually keywords are noun,\n",
    "adjectives or gerunds. \n",
    "\n",
    "Punctuations are added to the stopword list too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords = []\n",
    "\n",
    "# wanted_POS = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','VBG','FW'] \n",
    "\n",
    "# for word in POS_tag:\n",
    "#     if word[1] not in wanted_POS:\n",
    "#         stopwords.append(word[0])\n",
    "\n",
    "# punctuations = list(str(string.punctuation))\n",
    "\n",
    "# stopwords = stopwords + punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete stopword generation\n",
    "\n",
    "Even if we remove the aforementioned stopwords, still some extremely common nouns, adjectives or gerunds may\n",
    "remain which are very bad candidates for being keywords (or part of it). \n",
    "\n",
    "An external file constituting a long list of stopwords is loaded and all the words are added with the previous\n",
    "stopwords to create the final list 'stopwords-plus' which is then converted into a set. \n",
    "\n",
    "(Source of stopwords data: https://www.ranks.nl/stopwords)\n",
    "\n",
    "Stopwords-plus constitute the sum total of all stopwords and potential phrase-delimiters. \n",
    "\n",
    "(The contents of this set will be later used to partition the lemmatized text into n-gram phrases. But, for now, I will simply remove the stopwords, and work with a 'bag-of-words' approach. I will be developing the graph using unigram texts as vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'آباد',\n",
       " 'آره',\n",
       " 'آری',\n",
       " 'آمد',\n",
       " 'آمده',\n",
       " 'آن',\n",
       " 'آنان',\n",
       " 'آنجا',\n",
       " 'آنطور',\n",
       " 'آنقدر',\n",
       " 'آنكه',\n",
       " 'آنها',\n",
       " 'آنچه',\n",
       " 'آنکه',\n",
       " 'آورد',\n",
       " 'آورده',\n",
       " 'آيد',\n",
       " 'آی',\n",
       " 'آیا',\n",
       " 'آیند',\n",
       " 'اتفاقا',\n",
       " 'اثرِ',\n",
       " 'احتراما',\n",
       " 'احتمالا',\n",
       " 'اخیر',\n",
       " 'اری',\n",
       " 'از',\n",
       " 'ازجمله',\n",
       " 'اساسا',\n",
       " 'است',\n",
       " 'استفاد',\n",
       " 'استفاده',\n",
       " 'اش',\n",
       " 'اشکارا',\n",
       " 'اصلا',\n",
       " 'اصولا',\n",
       " 'اعلام',\n",
       " 'اغلب',\n",
       " 'اكنون',\n",
       " 'الان',\n",
       " 'البته',\n",
       " 'البتّه',\n",
       " 'ام',\n",
       " 'اما',\n",
       " 'امروز',\n",
       " 'امروزه',\n",
       " 'امسال',\n",
       " 'امشب',\n",
       " 'امور',\n",
       " 'ان',\n",
       " 'انجام',\n",
       " 'اند',\n",
       " 'انشاالله',\n",
       " 'انصافا',\n",
       " 'انطور',\n",
       " 'انقدر',\n",
       " 'انها',\n",
       " 'انچنان',\n",
       " 'انکه',\n",
       " 'انگار',\n",
       " 'او',\n",
       " 'اول',\n",
       " 'اولا',\n",
       " 'اي',\n",
       " 'ايشان',\n",
       " 'ايم',\n",
       " 'اين',\n",
       " 'اينكه',\n",
       " 'اکثرا',\n",
       " 'اکنون',\n",
       " 'اگر',\n",
       " 'ای',\n",
       " 'ایا',\n",
       " 'اید',\n",
       " 'ایشان',\n",
       " 'ایم',\n",
       " 'این',\n",
       " 'اینجا',\n",
       " 'ایند',\n",
       " 'اینطور',\n",
       " 'اینقدر',\n",
       " 'اینها',\n",
       " 'اینچنین',\n",
       " 'اینک',\n",
       " 'اینکه',\n",
       " 'اینگونه',\n",
       " 'با',\n",
       " 'بار',\n",
       " 'بارة',\n",
       " 'باره',\n",
       " 'بارها',\n",
       " 'باز',\n",
       " 'بازهم',\n",
       " 'باش',\n",
       " 'باشد',\n",
       " 'باشم',\n",
       " 'باشند',\n",
       " 'باشيم',\n",
       " 'باشی',\n",
       " 'باشید',\n",
       " 'باشیم',\n",
       " 'بالا',\n",
       " 'بالاخره',\n",
       " 'بالایِ',\n",
       " 'بالطبع',\n",
       " 'بايد',\n",
       " 'باید',\n",
       " 'بتوان',\n",
       " 'بتواند',\n",
       " 'بتوانی',\n",
       " 'بتوانیم',\n",
       " 'بخش',\n",
       " 'بخشی',\n",
       " 'بخواه',\n",
       " 'بخواهد',\n",
       " 'بخواهم',\n",
       " 'بخواهند',\n",
       " 'بخواهی',\n",
       " 'بخواهید',\n",
       " 'بخواهیم',\n",
       " 'بد',\n",
       " 'بدون',\n",
       " 'بر',\n",
       " 'برابر',\n",
       " 'برابرِ',\n",
       " 'براحتی',\n",
       " 'براساس',\n",
       " 'براستی',\n",
       " 'براي',\n",
       " 'برای',\n",
       " 'برایِ',\n",
       " 'برخوردار',\n",
       " 'برخي',\n",
       " 'برخی',\n",
       " 'برداري',\n",
       " 'برعکس',\n",
       " 'بروز',\n",
       " 'بزرگ',\n",
       " 'بزودی',\n",
       " 'بسا',\n",
       " 'بسيار',\n",
       " 'بسياري',\n",
       " 'بسیار',\n",
       " 'بسیاری',\n",
       " 'بطور',\n",
       " 'بعد',\n",
       " 'بعدا',\n",
       " 'بعدها',\n",
       " 'بعری',\n",
       " 'بعضا',\n",
       " 'بعضي',\n",
       " 'بلافاصله',\n",
       " 'بلكه',\n",
       " 'بله',\n",
       " 'بلکه',\n",
       " 'بلی',\n",
       " 'بنابراين',\n",
       " 'بنابراین',\n",
       " 'بندي',\n",
       " 'به',\n",
       " 'بهتر',\n",
       " 'بهترين',\n",
       " 'بود',\n",
       " 'بودم',\n",
       " 'بودن',\n",
       " 'بودند',\n",
       " 'بوده',\n",
       " 'بودی',\n",
       " 'بودید',\n",
       " 'بودیم',\n",
       " 'بویژه',\n",
       " 'بي',\n",
       " 'بيست',\n",
       " 'بيش',\n",
       " 'بيشتر',\n",
       " 'بيشتري',\n",
       " 'بين',\n",
       " 'بکن',\n",
       " 'بکند',\n",
       " 'بکنم',\n",
       " 'بکنند',\n",
       " 'بکنی',\n",
       " 'بکنید',\n",
       " 'بکنیم',\n",
       " 'بگو',\n",
       " 'بگوید',\n",
       " 'بگویم',\n",
       " 'بگویند',\n",
       " 'بگویی',\n",
       " 'بگویید',\n",
       " 'بگوییم',\n",
       " 'بگیر',\n",
       " 'بگیرد',\n",
       " 'بگیرم',\n",
       " 'بگیرند',\n",
       " 'بگیری',\n",
       " 'بگیرید',\n",
       " 'بگیریم',\n",
       " 'بی',\n",
       " 'بیا',\n",
       " 'بیاب',\n",
       " 'بیابد',\n",
       " 'بیابم',\n",
       " 'بیابند',\n",
       " 'بیابی',\n",
       " 'بیابید',\n",
       " 'بیابیم',\n",
       " 'بیاور',\n",
       " 'بیاورد',\n",
       " 'بیاورم',\n",
       " 'بیاورند',\n",
       " 'بیاوری',\n",
       " 'بیاورید',\n",
       " 'بیاوریم',\n",
       " 'بیاید',\n",
       " 'بیایم',\n",
       " 'بیایند',\n",
       " 'بیایی',\n",
       " 'بیایید',\n",
       " 'بیاییم',\n",
       " 'بیرون',\n",
       " 'بیرونِ',\n",
       " 'بیش',\n",
       " 'بیشتر',\n",
       " 'بیشتری',\n",
       " 'بین',\n",
       " 'ت',\n",
       " 'تا',\n",
       " 'تازه',\n",
       " 'تاكنون',\n",
       " 'تان',\n",
       " 'تاکنون',\n",
       " 'تحت',\n",
       " 'تر',\n",
       " 'تر  براساس',\n",
       " 'ترين',\n",
       " 'تقریبا',\n",
       " 'تلویحا',\n",
       " 'تمام',\n",
       " 'تماما',\n",
       " 'تمامي',\n",
       " 'تنها',\n",
       " 'تو',\n",
       " 'تواند',\n",
       " 'توانست',\n",
       " 'توانستم',\n",
       " 'توانستن',\n",
       " 'توانستند',\n",
       " 'توانسته',\n",
       " 'توانستی',\n",
       " 'توانستیم',\n",
       " 'توانم',\n",
       " 'توانند',\n",
       " 'توانی',\n",
       " 'توانید',\n",
       " 'توانیم',\n",
       " 'توسط',\n",
       " 'تولِ',\n",
       " 'تویِ',\n",
       " 'ثانیا',\n",
       " 'جا',\n",
       " 'جاي',\n",
       " 'جايي',\n",
       " 'جای',\n",
       " 'جدا',\n",
       " 'جديد',\n",
       " 'جدید',\n",
       " 'جريان',\n",
       " 'جریان',\n",
       " 'جز',\n",
       " 'جلوگيري',\n",
       " 'جلویِ',\n",
       " 'جمعا',\n",
       " 'جناح',\n",
       " 'جهت',\n",
       " 'حاضر',\n",
       " 'حال',\n",
       " 'حالا',\n",
       " 'حتما',\n",
       " 'حتي',\n",
       " 'حتی',\n",
       " 'حداکثر',\n",
       " 'حدودا',\n",
       " 'حدودِ',\n",
       " 'حق',\n",
       " 'خارجِ',\n",
       " 'خب',\n",
       " 'خدمات',\n",
       " 'خصوصا',\n",
       " 'خلاصه',\n",
       " 'خواست',\n",
       " 'خواستم',\n",
       " 'خواستن',\n",
       " 'خواستند',\n",
       " 'خواسته',\n",
       " 'خواستی',\n",
       " 'خواستید',\n",
       " 'خواستیم',\n",
       " 'خواهد',\n",
       " 'خواهم',\n",
       " 'خواهند',\n",
       " 'خواهيم',\n",
       " 'خواهی',\n",
       " 'خواهید',\n",
       " 'خواهیم',\n",
       " 'خوب',\n",
       " 'خود',\n",
       " 'خودت',\n",
       " 'خودتان',\n",
       " 'خودش',\n",
       " 'خودشان',\n",
       " 'خودم',\n",
       " 'خودمان',\n",
       " 'خوشبختانه',\n",
       " 'خويش',\n",
       " 'خویش',\n",
       " 'خویشتن',\n",
       " 'خیاه',\n",
       " 'خیر',\n",
       " 'خیلی',\n",
       " 'داد',\n",
       " 'دادم',\n",
       " 'دادن',\n",
       " 'دادند',\n",
       " 'داده',\n",
       " 'دادی',\n",
       " 'دادید',\n",
       " 'دادیم',\n",
       " 'دار',\n",
       " 'دارد',\n",
       " 'دارم',\n",
       " 'دارند',\n",
       " 'داريم',\n",
       " 'داری',\n",
       " 'دارید',\n",
       " 'داریم',\n",
       " 'داشت',\n",
       " 'داشتم',\n",
       " 'داشتن',\n",
       " 'داشتند',\n",
       " 'داشته',\n",
       " 'داشتی',\n",
       " 'داشتید',\n",
       " 'داشتیم',\n",
       " 'دانست',\n",
       " 'دانند',\n",
       " 'دایم',\n",
       " 'دایما',\n",
       " 'در',\n",
       " 'درباره',\n",
       " 'درمجموع',\n",
       " 'درون',\n",
       " 'دریغ',\n",
       " 'دقیقا',\n",
       " 'دنبالِ',\n",
       " 'ده',\n",
       " 'دهد',\n",
       " 'دهم',\n",
       " 'دهند',\n",
       " 'دهی',\n",
       " 'دهید',\n",
       " 'دهیم',\n",
       " 'دو',\n",
       " 'دوباره',\n",
       " 'دوم',\n",
       " 'ديده',\n",
       " 'ديروز',\n",
       " 'ديگر',\n",
       " 'ديگران',\n",
       " 'ديگري',\n",
       " 'دیر',\n",
       " 'دیروز',\n",
       " 'دیگر',\n",
       " 'دیگران',\n",
       " 'دیگری',\n",
       " 'را',\n",
       " 'راحت',\n",
       " 'راسا',\n",
       " 'راستی',\n",
       " 'راه',\n",
       " 'رسما',\n",
       " 'رسید',\n",
       " 'رفت',\n",
       " 'رفته',\n",
       " 'رو',\n",
       " 'روب',\n",
       " 'روز',\n",
       " 'روزانه',\n",
       " 'روزهاي',\n",
       " 'روي',\n",
       " 'روی',\n",
       " 'رویِ',\n",
       " 'ريزي',\n",
       " 'زمان',\n",
       " 'زمانی',\n",
       " 'زمینه',\n",
       " 'زود',\n",
       " 'زياد',\n",
       " 'زير',\n",
       " 'زيرا',\n",
       " 'زیر',\n",
       " 'زیرِ',\n",
       " 'سابق',\n",
       " 'ساخته',\n",
       " 'سازي',\n",
       " 'سالانه',\n",
       " 'سالیانه',\n",
       " 'سایر',\n",
       " 'سراسر',\n",
       " 'سرانجام',\n",
       " 'سریعا',\n",
       " 'سریِ',\n",
       " 'سعي',\n",
       " 'سمتِ',\n",
       " 'سوم',\n",
       " 'سوي',\n",
       " 'سوی',\n",
       " 'سویِ',\n",
       " 'سپس',\n",
       " 'شان',\n",
       " 'شايد',\n",
       " 'شاید',\n",
       " 'شخصا',\n",
       " 'شد',\n",
       " 'شدم',\n",
       " 'شدن',\n",
       " 'شدند',\n",
       " 'شده',\n",
       " 'شدی',\n",
       " 'شدید',\n",
       " 'شدیدا',\n",
       " 'شدیم',\n",
       " 'شش',\n",
       " 'شش  نداشته',\n",
       " 'شما',\n",
       " 'شناسي',\n",
       " 'شود',\n",
       " 'شوم',\n",
       " 'شوند',\n",
       " 'شونده',\n",
       " 'شوی',\n",
       " 'شوید',\n",
       " 'شویم',\n",
       " 'صرفا',\n",
       " 'صورت',\n",
       " 'ضدِّ',\n",
       " 'ضدِّ',\n",
       " 'ضمن',\n",
       " 'طبعا',\n",
       " 'طبقِ',\n",
       " 'طبیعتا',\n",
       " 'طرف',\n",
       " 'طريق',\n",
       " 'طریق',\n",
       " 'طور',\n",
       " 'طي',\n",
       " 'طی',\n",
       " 'ظاهرا',\n",
       " 'عدم',\n",
       " 'عقبِ',\n",
       " 'علّتِ',\n",
       " 'علیه',\n",
       " 'عمدا',\n",
       " 'عمدتا',\n",
       " 'عمل',\n",
       " 'عملا',\n",
       " 'عنوان',\n",
       " 'عنوانِ',\n",
       " 'غالبا',\n",
       " 'غير',\n",
       " 'غیر',\n",
       " 'فردا',\n",
       " 'فعلا',\n",
       " 'فقط',\n",
       " 'فكر',\n",
       " 'فوق',\n",
       " 'قابل',\n",
       " 'قبل',\n",
       " 'قبلا',\n",
       " 'قدری',\n",
       " 'قصدِ',\n",
       " 'قطعا',\n",
       " 'كرد',\n",
       " 'كردم',\n",
       " 'كردن',\n",
       " 'كردند',\n",
       " 'كرده',\n",
       " 'كسي',\n",
       " 'كل',\n",
       " 'كمتر',\n",
       " 'كند',\n",
       " 'كنم',\n",
       " 'كنند',\n",
       " 'كنيد',\n",
       " 'كنيم',\n",
       " 'كه',\n",
       " 'لااقل',\n",
       " 'لطفا',\n",
       " 'لطفاً',\n",
       " 'ما',\n",
       " 'مان',\n",
       " 'مانند',\n",
       " 'مانندِ',\n",
       " 'مبادا',\n",
       " 'متاسفانه',\n",
       " 'متعاقبا',\n",
       " 'مثل',\n",
       " 'مثلا',\n",
       " 'مثلِ',\n",
       " 'مجانی',\n",
       " 'مجددا',\n",
       " 'مجموعا',\n",
       " 'مختلف',\n",
       " 'مدام',\n",
       " 'مدت',\n",
       " 'مدّتی',\n",
       " 'مردم',\n",
       " 'مرسی',\n",
       " 'مستقیما',\n",
       " 'مسلما',\n",
       " 'مطمینا',\n",
       " 'معمولا',\n",
       " 'مقابل',\n",
       " 'ممکن',\n",
       " 'من',\n",
       " 'موارد',\n",
       " 'مورد',\n",
       " 'موقتا',\n",
       " 'مي',\n",
       " 'ميليارد',\n",
       " 'ميليون',\n",
       " 'مگر',\n",
       " 'می',\n",
       " 'می شود',\n",
       " 'میان',\n",
       " 'می\\u200cرسد',\n",
       " 'می\\u200cرود',\n",
       " 'می\\u200cشود',\n",
       " 'می\\u200cکنیم',\n",
       " 'ناشي',\n",
       " 'نام',\n",
       " 'ناگاه',\n",
       " 'ناگهان',\n",
       " 'ناگهانی',\n",
       " 'نبايد',\n",
       " 'نباید',\n",
       " 'نبود',\n",
       " 'نخست',\n",
       " 'نخستين',\n",
       " 'نخواهد',\n",
       " 'نخواهم',\n",
       " 'نخواهند',\n",
       " 'نخواهی',\n",
       " 'نخواهید',\n",
       " 'نخواهیم',\n",
       " 'ندارد',\n",
       " 'ندارم',\n",
       " 'ندارند',\n",
       " 'نداری',\n",
       " 'ندارید',\n",
       " 'نداریم',\n",
       " 'نداشت',\n",
       " 'نداشتم',\n",
       " 'نداشتند',\n",
       " 'نداشته',\n",
       " 'نداشتی',\n",
       " 'نداشتید',\n",
       " 'نداشتیم',\n",
       " 'نزديك',\n",
       " 'نزدِ',\n",
       " 'نزدیکِ',\n",
       " 'نسبتا',\n",
       " 'نشان',\n",
       " 'نشده',\n",
       " 'نظير',\n",
       " 'نظیر',\n",
       " 'نكرده',\n",
       " 'نمايد',\n",
       " 'نمي',\n",
       " 'نمی',\n",
       " 'نمی\\u200cشود',\n",
       " 'نه',\n",
       " 'نهایتا',\n",
       " 'نوع',\n",
       " 'نوعي',\n",
       " 'نوعی',\n",
       " 'نيز',\n",
       " 'نيست',\n",
       " 'نگاه',\n",
       " 'نیز',\n",
       " 'نیست',\n",
       " 'ها',\n",
       " 'هاي',\n",
       " 'هايي',\n",
       " 'های',\n",
       " 'هایی',\n",
       " 'هبچ',\n",
       " 'هر',\n",
       " 'هرچه',\n",
       " 'هرگز',\n",
       " 'هزار',\n",
       " 'هست',\n",
       " 'هستم',\n",
       " 'هستند',\n",
       " 'هستيم',\n",
       " 'هستی',\n",
       " 'هستید',\n",
       " 'هستیم',\n",
       " 'هفت',\n",
       " 'هم',\n",
       " 'همان',\n",
       " 'همه',\n",
       " 'همواره',\n",
       " 'همين',\n",
       " 'همچنان',\n",
       " 'همچنين',\n",
       " 'همچنین',\n",
       " 'همچون',\n",
       " 'همیشه',\n",
       " 'همین',\n",
       " 'هنوز',\n",
       " 'هنگام',\n",
       " 'هنگامِ',\n",
       " 'هنگامی',\n",
       " 'هيچ',\n",
       " 'هیچ',\n",
       " 'هیچگاه',\n",
       " 'و',\n",
       " 'واقعا',\n",
       " 'واقعی',\n",
       " 'وجود',\n",
       " 'وسطِ',\n",
       " 'وضع',\n",
       " 'وقتي',\n",
       " 'وقتی',\n",
       " 'وقتیکه',\n",
       " 'ولی',\n",
       " 'وي',\n",
       " 'وگو',\n",
       " 'وی',\n",
       " 'ویژه',\n",
       " 'يا',\n",
       " 'يابد',\n",
       " 'يك',\n",
       " 'يكديگر',\n",
       " 'يكي',\n",
       " 'ّه',\n",
       " '٪',\n",
       " 'پارسال',\n",
       " 'پاعینِ',\n",
       " 'پس',\n",
       " 'پنج',\n",
       " 'پيش',\n",
       " 'پیدا',\n",
       " 'پیش',\n",
       " 'پیشاپیش',\n",
       " 'پیشتر',\n",
       " 'پیشِ',\n",
       " 'چرا',\n",
       " 'چطور',\n",
       " 'چقدر',\n",
       " 'چنان',\n",
       " 'چنانچه',\n",
       " 'چنانکه',\n",
       " 'چند',\n",
       " 'چندین',\n",
       " 'چنين',\n",
       " 'چنین',\n",
       " 'چه',\n",
       " 'چهار',\n",
       " 'چو',\n",
       " 'چون',\n",
       " 'چيزي',\n",
       " 'چگونه',\n",
       " 'چیز',\n",
       " 'چیزی',\n",
       " 'چیست',\n",
       " 'کاش',\n",
       " 'کامل',\n",
       " 'کاملا',\n",
       " 'کتبا',\n",
       " 'کجا',\n",
       " 'کجاست',\n",
       " 'کدام',\n",
       " 'کرد',\n",
       " 'کردم',\n",
       " 'کردن',\n",
       " 'کردند',\n",
       " 'کرده',\n",
       " 'کردی',\n",
       " 'کردید',\n",
       " 'کردیم',\n",
       " 'کس',\n",
       " 'کسانی',\n",
       " 'کسی',\n",
       " 'کل',\n",
       " 'کلا',\n",
       " 'کم',\n",
       " 'کماکان',\n",
       " 'کمتر',\n",
       " 'کمتری',\n",
       " 'کمی',\n",
       " 'کن',\n",
       " 'کنار',\n",
       " 'کنارِ',\n",
       " 'کند',\n",
       " 'کنم',\n",
       " 'کنند',\n",
       " 'کننده',\n",
       " 'کنون',\n",
       " 'کنونی',\n",
       " 'کنی',\n",
       " 'کنید',\n",
       " 'کنیم',\n",
       " 'که',\n",
       " 'کو',\n",
       " 'کَی',\n",
       " 'کی',\n",
       " 'گاه',\n",
       " 'گاهی',\n",
       " 'گذاري',\n",
       " 'گذاشته',\n",
       " 'گذشته',\n",
       " 'گردد',\n",
       " 'گرفت',\n",
       " 'گرفتم',\n",
       " 'گرفتن',\n",
       " 'گرفتند',\n",
       " 'گرفته',\n",
       " 'گرفتی',\n",
       " 'گرفتید',\n",
       " 'گرفتیم',\n",
       " 'گروهي',\n",
       " 'گفت',\n",
       " 'گفتم',\n",
       " 'گفتن',\n",
       " 'گفتند',\n",
       " 'گفته',\n",
       " 'گفتی',\n",
       " 'گفتید',\n",
       " 'گفتیم',\n",
       " 'گه',\n",
       " 'گهگاه',\n",
       " 'گو',\n",
       " 'گويد',\n",
       " 'گويند',\n",
       " 'گویا',\n",
       " 'گوید',\n",
       " 'گویم',\n",
       " 'گویند',\n",
       " 'گویی',\n",
       " 'گویید',\n",
       " 'گوییم',\n",
       " 'گيرد',\n",
       " 'گيري',\n",
       " 'گیرد',\n",
       " 'گیرم',\n",
       " 'گیرند',\n",
       " 'گیری',\n",
       " 'گیرید',\n",
       " 'گیریم',\n",
       " 'ی',\n",
       " 'یا',\n",
       " 'یابد',\n",
       " 'یابم',\n",
       " 'یابند',\n",
       " 'یابی',\n",
       " 'یابید',\n",
       " 'یابیم',\n",
       " 'یافت',\n",
       " 'یافتم',\n",
       " 'یافتن',\n",
       " 'یافته',\n",
       " 'یافتی',\n",
       " 'یافتید',\n",
       " 'یافتیم',\n",
       " 'یعنی',\n",
       " 'یقینا',\n",
       " 'یه',\n",
       " 'یک',\n",
       " 'یکی'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = []\n",
    "stopword_file = open(\"resources/STOPWORDS.txt\", \"r\")\n",
    "#Source = https://www.ranks.nl/stopwords\n",
    "\n",
    "lots_of_stopwords = []\n",
    "\n",
    "for line in stopword_file.readlines():\n",
    "    lots_of_stopwords.append(str(line.strip()))\n",
    "\n",
    "stopwords_plus = []\n",
    "stopwords_plus = stopwords + lots_of_stopwords\n",
    "stopwords_plus = set(stopwords_plus)\n",
    "stopwords_plus\n",
    "#Stopwords_plus contain total set of all stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords \n",
    "\n",
    "Removing stopwords from lemmatized_text. \n",
    "Processeced_text condtains the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "در\n",
      "نمایشگاه\n",
      "کتاب\n",
      "بندرعباس\n",
      "آثار\n",
      "دونالد\n",
      "ترامپ\n",
      "در\n",
      "حوزه\n",
      "کتاب\n",
      "موفقیت\n",
      "به\n",
      "وفور\n",
      "عرضه\n",
      "شد#شو\n",
      ".\n",
      "این\n",
      "در\n",
      "حال\n",
      "#است\n",
      "که\n",
      "رئیس‌جمهور\n",
      "آمریکا\n",
      "مراکز\n",
      "فرهنگ\n",
      "ما\n",
      "را\n",
      "تهدید\n",
      "کرد#کن\n",
      "اما\n",
      "ما\n",
      "کتاب\n",
      "او\n",
      "را\n",
      "تبلیغ\n",
      "کرد#کن\n",
      "!\n",
      "]\n",
      "['نمایشگاه', 'کتاب', 'بندرعباس', 'آثار', 'دونالد', 'ترامپ', 'حوزه', 'کتاب', 'موفقیت', 'وفور', 'عرضه', 'شد#شو', '.', '#است', 'رئیس\\u200cجمهور', 'آمریکا', 'مراکز', 'فرهنگ', 'تهدید', 'کرد#کن', 'کتاب', 'تبلیغ', 'کرد#کن', '!', ']']\n"
     ]
    }
   ],
   "source": [
    "processed_text = []\n",
    "for word in lemmatized_text:\n",
    "    print(word)\n",
    "    if word not in stopwords_plus:\n",
    "        processed_text.append(word)\n",
    "print (processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Creation\n",
    "\n",
    "Vocabulary will only contain unique words from processed_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['شد#شو', 'تبلیغ', 'آمریکا', 'نمایشگاه', 'کتاب', '!', 'حوزه', '.', 'وفور', 'مراکز', 'موفقیت', '#است', 'رئیس\\u200cجمهور', 'دونالد', 'کرد#کن', 'عرضه', 'ترامپ', 'تهدید', 'فرهنگ', 'آثار', 'بندرعباس', ']']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list(set(processed_text))\n",
    "print (vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Graph\n",
    "\n",
    "TextRank is a graph based model, and thus it requires us to build a graph. Each words in the vocabulary will serve as a vertex for graph. The words will be represented in the vertices by their index in vocabulary list.  \n",
    "\n",
    "The weighted_edge matrix contains the information of edge connections among all vertices.\n",
    "I am building wieghted undirected edges.\n",
    "\n",
    "weighted_edge[i][j] contains the weight of the connecting edge between the word vertex represented by vocabulary index i and the word vertex represented by vocabulary j.\n",
    "\n",
    "If weighted_edge[i][j] is zero, it means no edge connection is present between the words represented by index i and j.\n",
    "\n",
    "There is a connection between the words (and thus between i and j which represents them) if the words co-occur within a window of a specified 'window_size' in the processed_text.\n",
    "\n",
    "The value of the weighted_edge[i][j] is increased by (1/(distance between positions of words currently represented by i and j)) for every connection discovered between the same words in different locations of the text. \n",
    "\n",
    "The covered_coocurrences list (which is contain the list of pairs of absolute positions in processed_text of the words whose coocurrence at that location is already checked) is managed so that the same two words located in the same positions in processed_text are not repetitively counted while sliding the window one text unit at a time.\n",
    "\n",
    "The score of all vertices are intialized to one. \n",
    "\n",
    "Self-connections are not considered, so weighted_edge[i][i] will be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "vocab_len = len(vocabulary)\n",
    "\n",
    "weighted_edge = np.zeros((vocab_len,vocab_len),dtype=np.float32)\n",
    "\n",
    "score = np.zeros((vocab_len),dtype=np.float32)\n",
    "window_size = 3\n",
    "covered_coocurrences = []\n",
    "\n",
    "for i in range(0,vocab_len):\n",
    "    score[i]=1\n",
    "    for j in range(0,vocab_len):\n",
    "        if j==i:\n",
    "            weighted_edge[i][j]=0\n",
    "        else:\n",
    "            for window_start in range(0,(len(processed_text)-window_size)):\n",
    "                \n",
    "                window_end = window_start+window_size\n",
    "                \n",
    "                window = processed_text[window_start:window_end]\n",
    "                \n",
    "                if (vocabulary[i] in window) and (vocabulary[j] in window):\n",
    "                    \n",
    "                    index_of_i = window_start + window.index(vocabulary[i])\n",
    "                    index_of_j = window_start + window.index(vocabulary[j])\n",
    "                    \n",
    "                    # index_of_x is the absolute position of the xth term in the window \n",
    "                    # (counting from 0) \n",
    "                    # in the processed_text\n",
    "                      \n",
    "                    if [index_of_i,index_of_j] not in covered_coocurrences:\n",
    "                        weighted_edge[i][j]+=1/math.fabs(index_of_i-index_of_j)\n",
    "                        covered_coocurrences.append([index_of_i,index_of_j])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating weighted summation of connections of a vertex\n",
    "\n",
    "inout[i] will contain the sum of all the undirected connections\\edges associated withe the vertex represented by i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inout = np.zeros((vocab_len),dtype=np.float32)\n",
    "\n",
    "for i in range(0,vocab_len):\n",
    "    for j in range(0,vocab_len):\n",
    "        inout[i]+=weighted_edge[i][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Vertices\n",
    "\n",
    "The formula used for scoring a vertex represented by i is:\n",
    "\n",
    "score[i] = (1-d) + d x [ Summation(j) ( (weighted_edge[i][j]/inout[j]) x score[j] ) ] where j belongs to the list of vertieces that has a connection with i. \n",
    "\n",
    "d is the damping factor.\n",
    "\n",
    "The score is iteratively updated until convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converging at iteration 16....\n"
     ]
    }
   ],
   "source": [
    "MAX_ITERATIONS = 50\n",
    "d=0.85\n",
    "threshold = 0.0001 #convergence threshold\n",
    "\n",
    "for iter in range(0,MAX_ITERATIONS):\n",
    "    prev_score = np.copy(score)\n",
    "    \n",
    "    for i in range(0,vocab_len):\n",
    "        \n",
    "        summation = 0\n",
    "        for j in range(0,vocab_len):\n",
    "            if weighted_edge[i][j] != 0:\n",
    "                summation += (weighted_edge[i][j]/inout[j])*score[j]\n",
    "                \n",
    "        score[i] = (1-d) + d*(summation)\n",
    "    \n",
    "    if np.sum(np.fabs(prev_score-score)) <= threshold: #convergence condition\n",
    "        print(\"Converging at iteration \"+str(iter)+\"....\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of شد#شو: 0.9691715\n",
      "Score of تبلیغ: 0.89959514\n",
      "Score of آمریکا: 0.96968466\n",
      "Score of نمایشگاه: 0.5185362\n",
      "Score of کتاب: 2.3692758\n",
      "Score of !: 0.52083963\n",
      "Score of حوزه: 0.91073686\n",
      "Score of .: 0.97555435\n",
      "Score of وفور: 0.9371586\n",
      "Score of مراکز: 0.9586738\n",
      "Score of موفقیت: 0.9172329\n",
      "Score of #است: 0.9775365\n",
      "Score of رئیس‌جمهور: 0.97572833\n",
      "Score of دونالد: 0.93479085\n",
      "Score of کرد#کن: 1.5749172\n",
      "Score of عرضه: 0.95827174\n",
      "Score of ترامپ: 0.9227248\n",
      "Score of تهدید: 0.9139028\n",
      "Score of فرهنگ: 0.93963355\n",
      "Score of آثار: 0.92725676\n",
      "Score of بندرعباس: 0.92899764\n",
      "Score of ]: 0.15\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,vocab_len):\n",
    "    print(\"Score of \"+vocabulary[i]+\": \"+str(score[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase Partiotioning\n",
    "\n",
    "Paritioning lemmatized_text into phrases using the stopwords in it as delimeters.\n",
    "The phrases are also candidates for keyphrases to be extracted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioned Phrases (Candidate Keyphrases): \n",
      "\n",
      "[['نمایشگاه', 'کتاب', 'بندرعباس', 'آثار', 'دونالد', 'ترامپ'], ['حوزه', 'کتاب', 'موفقیت'], ['وفور', 'عرضه', 'شد#شو', '.'], ['#است'], ['رئیس\\u200cجمهور', 'آمریکا', 'مراکز', 'فرهنگ'], ['تهدید', 'کرد#کن'], ['کتاب']]\n"
     ]
    }
   ],
   "source": [
    "phrases = []\n",
    "\n",
    "phrase = \" \"\n",
    "for word in lemmatized_text:\n",
    "    \n",
    "    if word in stopwords_plus:\n",
    "        if phrase!= \" \":\n",
    "            phrases.append(str(phrase).strip().split())\n",
    "        phrase = \" \"\n",
    "    elif word not in stopwords_plus:\n",
    "        phrase+=str(word)\n",
    "        phrase+=\" \"\n",
    "\n",
    "print(\"Partitioned Phrases (Candidate Keyphrases): \\n\")\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of unique phrases.\n",
    "\n",
    "Repeating phrases\\keyphrase candidates has no purpose here, anymore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Phrases (Candidate Keyphrases): \n",
      "\n",
      "[['نمایشگاه', 'کتاب', 'بندرعباس', 'آثار', 'دونالد', 'ترامپ'], ['حوزه', 'کتاب', 'موفقیت'], ['وفور', 'عرضه', 'شد#شو', '.'], ['#است'], ['رئیس\\u200cجمهور', 'آمریکا', 'مراکز', 'فرهنگ'], ['تهدید', 'کرد#کن'], ['کتاب']]\n"
     ]
    }
   ],
   "source": [
    "unique_phrases = []\n",
    "\n",
    "for phrase in phrases:\n",
    "    if phrase not in unique_phrases:\n",
    "        unique_phrases.append(phrase)\n",
    "\n",
    "print(\"Unique Phrases (Candidate Keyphrases): \\n\")\n",
    "print(unique_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinning the list of candidate-keyphrases.\n",
    "\n",
    "Removing single word keyphrases-candidates that are present multi-word alternatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinned Unique Phrases (Candidate Keyphrases): \n",
      "\n",
      "[['نمایشگاه', 'کتاب', 'بندرعباس', 'آثار', 'دونالد', 'ترامپ'], ['حوزه', 'کتاب', 'موفقیت'], ['وفور', 'عرضه', 'شد#شو', '.'], ['#است'], ['رئیس\\u200cجمهور', 'آمریکا', 'مراکز', 'فرهنگ'], ['تهدید', 'کرد#کن']]\n"
     ]
    }
   ],
   "source": [
    "for word in vocabulary:\n",
    "    #print word\n",
    "    for phrase in unique_phrases:\n",
    "        if (word in phrase) and ([word] in unique_phrases) and (len(phrase)>1):\n",
    "            #if len(phrase)>1 then the current phrase is multi-worded.\n",
    "            #if the word in vocabulary is present in unique_phrases as a single-word-phrase\n",
    "            # and at the same time present as a word within a multi-worded phrase,\n",
    "            # then I will remove the single-word-phrase from the list.\n",
    "            unique_phrases.remove([word])\n",
    "            \n",
    "print(\"Thinned Unique Phrases (Candidate Keyphrases): \\n\")\n",
    "print(unique_phrases)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Keyphrases\n",
    "\n",
    "Scoring the phrases (candidate keyphrases) and building up a list of keyphrases\\keywords\n",
    "by listing untokenized versions of tokenized phrases\\candidate-keyphrases.\n",
    "Phrases are scored by adding the score of their members (words\\text-units that were ranked by the graph algorithm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   ('نمایشگاه کتاب بندرعباس آثار دونالد ترامپ', 6.601582050323486)\n",
      "1   ('حوزه کتاب موفقیت', 4.197245538234711)\n",
      "2   ('وفور عرضه شد#شو .', 3.8401561975479126)\n",
      "3   ('#است', 0.9775364995002747)\n",
      "4   ('رئیس\\u200cجمهور آمریکا مراکز فرهنگ', 3.843720316886902)\n",
      "5   ('تهدید کرد#کن', 2.4888200163841248)\n"
     ]
    }
   ],
   "source": [
    "phrase_scores = []\n",
    "keywords = []\n",
    "for phrase in unique_phrases:\n",
    "    phrase_score=0\n",
    "    keyword = ''\n",
    "    for word in phrase:\n",
    "        keyword += str(word)\n",
    "        keyword += \" \"\n",
    "        phrase_score+=score[vocabulary.index(word)]\n",
    "    phrase_scores.append(phrase_score)\n",
    "    keywords.append(keyword.strip())\n",
    "\n",
    "i=0\n",
    "words_with_scores = []\n",
    "for keyword in keywords:\n",
    "#     print(words_with_scores)\n",
    "    tup = (keyword,phrase_scores[i])\n",
    "    words_with_scores.append(tup)\n",
    "#     print (\"Keyword: '\"+str(keyword)+\"', Score: \"+str(phrase_scores[i]))\n",
    "    i+=1\n",
    " \n",
    "# print(len(words_with_scores))\n",
    "# print(word_with_scores)\n",
    "for i,ws in enumerate(words_with_scores):\n",
    "    print(f\"{i}\",ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking Keyphrases\n",
    "\n",
    "Ranking keyphrases based on their calculated scores. Displaying top keywords_num no. of keyphrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.601582050323486, 4.197245538234711, 3.8401561975479126, 0.9775364995002747, 3.843720316886902, 2.4888200163841248]\n",
      "Keywords:\n",
      "\n",
      "0 نمایشگاه کتاب بندرعباس آثار دونالد ترامپ,  1 حوزه کتاب موفقیت,  2 رئیس‌جمهور آمریکا مراکز فرهنگ,  3 وفور عرضه شد#شو .,  4 تهدید کرد#کن,  5 #است,  "
     ]
    }
   ],
   "source": [
    "sorted_index = np.flip(np.argsort(phrase_scores),0)\n",
    "\n",
    "print(phrase_scores)\n",
    "\n",
    "keywords_num = 10\n",
    "\n",
    "print(\"Keywords:\\n\")\n",
    "\n",
    "for i in range(0,keywords_num):\n",
    "    try:\n",
    "        print(str(i),str(keywords[sorted_index[i]])+\", \", end=' ')\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input:\n",
    "\n",
    "Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types of systems and systems of mixed types.\n",
    "\n",
    "# Extracted Keywords:\n",
    "\n",
    "* minimal supporting set,  \n",
    "* minimal generating set,  \n",
    "* minimal set,  \n",
    "* linear diophantine equation,  \n",
    "* nonstrict inequations,  \n",
    "* strict inequations,  \n",
    "* system,  \n",
    "* linear constraint,  \n",
    "* solution,  \n",
    "* upper bound, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
